{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d52cb47",
   "metadata": {},
   "source": [
    "# Lab 5: Statistical Distributions in Behavioral Genetics & Substance Use\n",
    "\n",
    "**Learning Objectives**  \n",
    "By the end of this lab, you will be able to:  \n",
    "\n",
    "1. **Standardize continuous variables and assess normality** using z-scores and Q-Q plots (quantile‚Äìquantile plots).  \n",
    "2. **Calculate probabilities from a normal distribution** for standardized behavioral measures.  \n",
    "3. **Apply the binomial distribution** to model probabilities of binary outcomes (e.g. substance use) in small samples.  \n",
    "4. **Use the Poisson distribution** to simulate counts of rare events (e.g. opioid overdoses) in a population given a known rate.  \n",
    "5. **Simulate Bernoulli trials** to understand single-event outcomes (e.g. whether an individual has used a substance) and their probabilities.\n",
    "\n",
    "## Setup\n",
    "\n",
    "Run the setup cell below to import libraries and load the dataset for Lab 4/5 (a synthetic ABCD-derived dataset of ~11,000 young adults). This dataset includes cognitive measures, polygenic scores (PGS), and substance use indicators. We‚Äôll use pandas for data handling, numpy for calculations, and matplotlib for plotting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00186a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Configure display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "FIG_DIR = Path(\"figures\")\n",
    "FIG_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "def savefig(name, dpi=150):\n",
    "    \"\"\"Save current Matplotlib figure to ./figures/<name>.png\"\"\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(FIG_DIR / name, dpi=dpi)\n",
    "    plt.close()\n",
    "\n",
    "# Load the Lab4/Lab5 dataset\n",
    "df = pd.read_csv('___') # TODO: insert the dataset filename here\n",
    "\n",
    "# Display shape and first few rows\n",
    "print(f\"Dataset contains {df.shape[0]} rows and {df.shape[1]} columns.\")\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6babb53f",
   "metadata": {},
   "source": [
    "### Activity 1. Normal Distribution I: Standardization and Q-Q Plots\n",
    "\n",
    "**Demo:** In this first activity, we examine a cognitive measure and test if it follows a **normal distribution**. We‚Äôll use the NIH Toolbox Cognition **Composite Score** (`nc_y_nihtb__comp__tot__agecor_score`), which is an age-corrected cognitive performance score (expected to be roughly normally distributed with mean around 100). We will **standardize** this variable to z-scores and create a **Q-Q plot** to check normality. \n",
    "\n",
    "*Standardization* means converting values to units of standard deviations from the mean: `z = (x ‚Äì mean) / std`. This yields a distribution with mean 0 and SD 1. After standardizing, a **Q-Q plot** (quantile‚Äìquantile plot) will compare the sorted data values (quantiles) to theoretical quantiles from a standard normal distribution. If the cognitive scores are normally distributed, the points in the Q-Q plot should lie approximately on a straight diagonal line.\n",
    "\n",
    "First, let‚Äôs compute the mean and standard deviation of the composite scores, convert them to z-scores, and output a few examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a0711",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the NIH Toolbox composite score and drop any missing values (including 999 as missing)\n",
    "scores = df['nc_y_nihtb__comp__tot__agecor_score']\n",
    "scores = scores[(scores != 999) & (~scores.isna())]\n",
    "\n",
    "# Calculate mean and standard deviation\n",
    "mean_score = scores.mean()\n",
    "std_score = scores.std()\n",
    "\n",
    "# Standardize to z-scores\n",
    "z_scores = (scores - mean_score) / std_score\n",
    "\n",
    "print(f\"Mean (raw composite) = {mean_score:.2f}, SD = {std_score:.2f}\")\n",
    "print(\"First 5 z-scores:\", np.round(z_scores.head(5), 2).tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1dbd9e",
   "metadata": {},
   "source": [
    "Now we‚Äôll plot a Q-Q plot for the z-scores. We expect the points to roughly follow the $y=x$ line if the data are normal. We use `scipy.stats.probplot` to generate the plot:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f084505",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Create a Q-Q plot of the z-scores against a theoretical normal distribution\n",
    "plt.figure()\n",
    "stats.probplot(z_scores, dist=\"norm\", plot=plt)\n",
    "plt.title(\"Normal Q-Q Plot: NIH Toolbox Composite Score\")\n",
    "plt.xlabel(\"Theoretical Quantiles\")\n",
    "plt.ylabel(\"Sample Quantiles\")\n",
    "savefig(\"qq_plot_nih_toolbox_composite_score.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd717009",
   "metadata": {},
   "source": [
    "### Interpreting this Normal Q‚ÄìQ plot\n",
    "\n",
    "**What a Q‚ÄìQ plot shows:** If the dots fall on the red line, the data look like a Normal (bell-shaped) distribution. Systematic bends or flat ‚Äúshelves‚Äù mean the data depart from Normal.\n",
    "\n",
    "**What we see here:**\n",
    "\n",
    "* **Center looks good.** Most points in the middle track the red line closely ‚Üí the bulk of scores are roughly bell-shaped.\n",
    "* **Right side ‚Äúshelf.‚Äù** The dots flatten out around ~2 on the vertical axis (a little horizontal plateau). That‚Äôs a **ceiling effect**‚Äîmany people are hitting or bunching near a top score, so the upper tail is **lighter/shorter** than a true Normal.\n",
    "* **Left tail dip.** A few low values sit below the line (around ‚àí3 to ‚àí3.5), suggesting a small departure at the very low end (possible floor clumping or mild skew).\n",
    "\n",
    "**What this means for analysis:**\n",
    "\n",
    "* For typical summaries and many tests, it‚Äôs fine to report **mean and SD** (the center behaves well).\n",
    "* Because the tails aren‚Äôt perfectly Normal‚Äîespecially the **ceiling**‚Äîalso report a **median and IQR**, and be cautious drawing conclusions about **extreme** scores.\n",
    "* If your question depends on the high end (e.g., distinguishing top performers), note the ceiling effect and consider a robustness check (e.g., trimmed mean) or simply state the limitation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed6a4b3c",
   "metadata": {},
   "source": [
    "### Your Turn: NIH Toolbox vs. Cognitive PGS Normality\n",
    "\n",
    "Now it‚Äôs your turn to repeat this process with a different variable: the **Cognitive Polygenic Score** (`pgs_cog_std`). This variable is a standardized polygenic score (PGS) for cognitive ability (higher values might indicate higher genetic propensity for cognitive performance). \n",
    "\n",
    "Follow these steps:  \n",
    "1. Calculate the mean and standard deviation of `pgs_cog_std` (note: since it‚Äôs labeled `_std`, it may already have mean ~0 and SD ~1, but you can verify).  \n",
    "2. Convert the PGS values to z-scores (or if already standardized, you can use them as-is).  \n",
    "3. Create a Q-Q plot for `pgs_cog_std` similar to the demo above.  \n",
    "4. Examine the Q-Q plot to determine if the cognitive PGS appears normally distributed (do the points fall on a straight line, or do you see systematic curves/deviations?).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daad3d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT1.1 QQ Plot for Cognitive PGS\n",
    "\n",
    "# Step 1. Extract and clean (TODO: students fill the column name)\n",
    "pgs = df['___']   # TODO: replace ___ with 'pgs_cog_std'\n",
    "pgs = pgs[(pgs != 999) & (~pgs.isna())]\n",
    "\n",
    "# Step 2. Standardize (fill in formula)\n",
    "mean_pgs = pgs.mean()\n",
    "std_pgs = pgs.std(ddof=1)\n",
    "z_pgs = (pgs - ___) / ___   # TODO: subtract mean, divide by sd\n",
    "\n",
    "print(f\"Mean (pgs_cog_std) = {mean_pgs:.2f}, SD = {std_pgs:.2f}\")\n",
    "\n",
    "# Step 3. QQ plot (students write the actual plot commands)\n",
    "plt.figure()\n",
    "# TODO: call scipy.stats.probplot with z_pgs, dist=\"norm\", and plot=plt\n",
    "# TODO: set a title and axis labels\n",
    "# Example titles/labels: \n",
    "#   Title: \"QQ Plot ‚Äî Cognitive PGS (z)\"\n",
    "#   X-axis: \"Theoretical Quantiles\"\n",
    "#   Y-axis: \"Sample Quantiles\"\n",
    "\n",
    "# TODO: save figure with filename \"Q1a_QQ_pgs_z.png\"\n",
    "plt.show()\n",
    "\n",
    "# Step 4. Print a quick check of first 5 z-scores (to confirm they look standardized)\n",
    "print(\"First 5 z-scores:\", np.round(z_pgs.head(5), 2).tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01edbbd5",
   "metadata": {},
   "source": [
    "This code calculates the proportion of individuals with z-scores above 1.65 (top ~5%) for both the NIH Toolbox composite and the cognitive PGS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96a87c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Proportion above z = 1.65 (~top 5%) for both variables\n",
    "z_cut = 1.65\n",
    "prop_above_demo = (z_scores >= z_cut).mean()\n",
    "prop_above_yourturn = (z_pgs >= z_cut).mean()\n",
    "print(\"Proportion above z=1.65 (NIH Toolbox):\", round(prop_above_demo, 3))\n",
    "print(\"Proportion above z=1.65 (PGS):\", round(prop_above_yourturn, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bd7ea8",
   "metadata": {},
   "source": [
    "### Exploring the Relationship: Bivariate EDA of Cognitive PGS and NIH Toolbox Composite\n",
    "\n",
    "Now that we've looked at each variable separately, let's explore how they relate to each other. This step is called **bivariate exploratory data analysis (EDA)**. We'll examine whether individuals with higher cognitive polygenic scores (PGS) also tend to have higher cognitive performance (as measured by the NIH Toolbox Composite Score).\n",
    "\n",
    "Here's what we'll do:\n",
    "- **Clean and standardize** both variables to ensure they're on the same scale and missing values are handled.\n",
    "- **Calculate correlations** (Pearson and Spearman) to quantify the strength and direction of the association.\n",
    "- **Visualize the relationship** with a scatterplot, showing how cognitive PGS and performance move together across individuals.\n",
    "- **Summarize trends** by plotting average performance within each decile (10% group) of PGS, giving a clearer sense of how performance changes across the PGS spectrum.\n",
    "\n",
    "This analysis helps us see not just if there's a relationship, but also how strong it is and what its pattern looks like across the range of genetic scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c875cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Bivariate EDA: Cognitive PGS vs NIH Toolbox Composite ---\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "# 1) Clean and standardize both variables\n",
    "x = df['pgs_cog_std']\n",
    "y = df['nc_y_nihtb__comp__tot__agecor_score']\n",
    "\n",
    "# Treat 999 as missing if present, then drop NaNs\n",
    "mask = (~x.isna()) & (x != 999) & (~y.isna()) & (y != 999)\n",
    "x = x[mask].astype(float)\n",
    "y = y[mask].astype(float)\n",
    "\n",
    "# Standardize outcome (PGS is already ~std, but standardize both for symmetry)\n",
    "zx = (x - x.mean()) / x.std()\n",
    "zy = (y - y.mean()) / y.std()\n",
    "\n",
    "# 2) Correlations\n",
    "pearson_r, pearson_p = stats.pearsonr(zx, zy)\n",
    "spearman_rho, spearman_p = stats.spearmanr(zx, zy)\n",
    "\n",
    "print(f\"Pearson r = {pearson_r:.3f} (p = {pearson_p:.3g})\")\n",
    "print(f\"Spearman œÅ = {spearman_rho:.3f} (p = {spearman_p:.3g})\")\n",
    "print(\"Interpretation tip: with both variables z-scored, Pearson r ‚âà SD change in NIH score per 1 SD of PGS.\")\n",
    "\n",
    "# 3) Scatter with light alpha\n",
    "plt.figure()\n",
    "plt.scatter(zx, zy, s=10, alpha=0.25)\n",
    "plt.xlabel(\"Cognitive PGS (z)\")\n",
    "plt.ylabel(\"NIH Toolbox Composite (z)\")\n",
    "plt.title(\"PGS vs Cognitive Performance (z‚Äìz scatter)\")\n",
    "plt.axhline(0, lw=0.5)\n",
    "plt.axvline(0, lw=0.5)\n",
    "plt.show()\n",
    "\n",
    "# 4) Quantile-bin means (deciles of PGS) as a simple trend summary\n",
    "q = pd.qcut(zx, 10, labels=False, duplicates='drop')\n",
    "decile_means = pd.DataFrame({\"zx\": zx, \"zy\": zy, \"q\": q}).groupby(\"q\").agg(\n",
    "    mean_pgs=(\"zx\",\"mean\"),\n",
    "    mean_perf=(\"zy\",\"mean\"),\n",
    "    se_perf=(\"zy\", lambda s: s.std(ddof=1)/np.sqrt(len(s)))\n",
    ").reset_index()\n",
    "\n",
    "plt.figure()\n",
    "plt.errorbar(decile_means[\"mean_pgs\"], decile_means[\"mean_perf\"],\n",
    "             yerr=decile_means[\"se_perf\"], fmt='o-', capsize=3)\n",
    "plt.xlabel(\"Cognitive PGS (decile means, z)\")\n",
    "plt.ylabel(\"NIH Toolbox Composite (decile means, z)\")\n",
    "plt.title(\"Decile plot: mean performance by PGS decile (¬±SE)\")\n",
    "plt.axhline(0, lw=0.5)\n",
    "plt.axvline(0, lw=0.5)\n",
    "savefig(\"decile_plot_pgs_vs_nih_toolbox.png\")\n",
    "plt.show()\n",
    "\n",
    "# 5) Quick takeaway string\n",
    "slope_like = pearson_r  # with z‚Äìz, the regression slope would equal r\n",
    "print(f\"Quick takeaway: A 1 SD increase in PGS is associated with about {slope_like:.2f} SD higher performance (by Pearson r).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40da84d6",
   "metadata": {},
   "source": [
    "### From visualization to summary numbers\n",
    "\n",
    "The decile plot gave us an intuitive picture: as cognitive PGS increases, average NIH Toolbox scores also increase in a fairly smooth, monotonic way.\n",
    "\n",
    "To communicate this relationship clearly, it helps to pair two complementary summaries:\n",
    "\n",
    "- **Pearson r** ‚Äî a compact statistic showing how strongly two standardized variables move together.\n",
    "- **Œî‚ÇÅ‚ÇÄ‚Äì‚ÇÅ (decile gap)** ‚Äî the average difference in outcome between the top and bottom deciles of PGS, which provides an intuitive effect size.\n",
    "\n",
    "In the next code cell, we‚Äôll calculate both side by side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852912c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assumes df is loaded; 999 marks non-response; both vars exist.\n",
    "# This cell recomputes everything in case prior cells weren‚Äôt run.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "x = df['pgs_cog_std']\n",
    "y = df['nc_y_nihtb__comp__tot__agecor_score']\n",
    "\n",
    "mask = (~x.isna()) & (x != 999) & (~y.isna()) & (y != 999)\n",
    "x = x[mask].astype(float)\n",
    "y = y[mask].astype(float)\n",
    "\n",
    "# z‚Äìz scaling for interpretability\n",
    "zx = (x - x.mean()) / x.std(ddof=1)\n",
    "zy = (y - y.mean()) / y.std(ddof=1)\n",
    "\n",
    "# 1) Correlations\n",
    "pearson_r, pearson_p = stats.pearsonr(zx, zy)\n",
    "spearman_rho, spearman_p = stats.spearmanr(zx, zy)\n",
    "\n",
    "# 2) Decile gap Œî10‚Äì1 (mean outcome in top vs bottom PGS decile)\n",
    "q = pd.qcut(zx, 10, labels=False, duplicates='drop')  # 0..9\n",
    "dm = pd.DataFrame({'zx': zx, 'zy': zy, 'q': q}).groupby('q')['zy'].mean()\n",
    "delta_10_1 = dm.loc[dm.index.max()] - dm.loc[dm.index.min()]\n",
    "\n",
    "print(f\"Pearson r = {pearson_r:.3f} (p={pearson_p:.3g})\")\n",
    "print(f\"Spearman œÅ = {spearman_rho:.3f} (p={spearman_p:.3g})\")\n",
    "print(f\"Œî10‚Äì1 (top‚Äìbottom decile gap, in SD) = {delta_10_1:.2f}\")\n",
    "\n",
    "print(\n",
    "    \"One-liner: PGS and performance are positively related \"\n",
    "    f\"(r={pearson_r:.2f}); top-decile vs bottom-decile differs by \"\n",
    "    f\"‚âà {delta_10_1:.2f} SD.\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b6b397",
   "metadata": {},
   "source": [
    "### Communicating the association: report **r** + Œî‚ÇÅ‚ÇÄ‚Äì‚ÇÅ (decile gap)\n",
    "\n",
    "**Why both?**\n",
    "\n",
    "- **Pearson r** (with both variables z-scored) is a compact, unitless summary of linear association. Interpretable as: ‚ÄúSD change in performance per 1 SD of PGS.‚Äù\n",
    "- **Œî‚ÇÅ‚ÇÄ‚Äì‚ÇÅ** is the top‚Äìbottom decile gap in outcome (in SD units if you z-scored the outcome). It‚Äôs an intuitive ‚Äúdistance‚Äù for non-technical audiences.\n",
    "\n",
    "**How to report (1‚Äì2 sentences):**\n",
    "\n",
    "> ‚ÄúPGS and performance are positively related (Pearson r = ___). On average, students in the top PGS decile score ___ SD higher than those in the bottom decile (Œî‚ÇÅ‚ÇÄ‚Äì‚ÇÅ = ___ SD).‚Äù"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86262e6a",
   "metadata": {},
   "source": [
    "### Activity 2. Normal Distribution II: Normal Probabilities\n",
    "\n",
    "**Demo:** Next, we will use the normal distribution to answer probability questions about a behavioral measure. Specifically, let‚Äôs consider a **delay discounting** measure, `nc_y_ddis__1mo__indifpt_prop`, which represents a 1-month delay discounting indifference point (a proportion indicating how much of a reward a person would take now vs. in 1 month). This is a continuous measure of impulsivity. We‚Äôll assume (after standardization) that it roughly follows a normal distribution. Using the **standard normal CDF**, we can find probabilities of certain outcomes (e.g. what fraction of individuals fall above or below a certain threshold).\n",
    "\n",
    "First, standardize the delay discounting score to get z-scores (mean 0, SD 1):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2e41f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delay discounting 1-month proportion score\n",
    "dd_score = df['nc_y_ddis__1mo__indifpt_prop'].dropna()\n",
    "mean_dd = dd_score.mean()\n",
    "std_dd = dd_score.std()\n",
    "dd_z = (dd_score - mean_dd) / std_dd\n",
    "\n",
    "print(f\"Mean delay discounting score = {mean_dd:.3f}, SD = {std_dd:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fced74",
   "metadata": {},
   "source": [
    "Now, using the standard normal distribution, we can calculate some probabilities. For example, **what proportion of people have a delay discounting z-score greater than 1?** (i.e., more than one standard deviation above the mean, indicating unusually high impulsivity). Similarly, we can find the probability of being **lower than ‚Äì1** (very low impulsivity) or beyond **+2** (extremely high impulsivity). We‚Äôll use `scipy.stats.norm.cdf` for cumulative probabilities:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617569a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Probability of being more than 1 SD above the mean (Z > 1)\n",
    "prob_above1 = 1 - norm.cdf(1)   # = P(Z > 1) = 1 - P(Z <= 1)\n",
    "# Probability of being more than 1 SD below the mean (Z < -1)\n",
    "prob_below_neg1 = norm.cdf(-1)  # = P(Z < -1)\n",
    "# Probability of being more than 2 SD above the mean (Z > 2)\n",
    "prob_above2 = 1 - norm.cdf(2)\n",
    "\n",
    "print(f\"P(Z > 1)  = {prob_above1:.3f} (proportion above +1 SD)\")\n",
    "print(f\"P(Z < -1) = {prob_below_neg1:.3f} (proportion below -1 SD)\")\n",
    "print(f\"P(Z > 2)  = {prob_above2:.3f} (proportion above +2 SD)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6de56b",
   "metadata": {},
   "source": [
    "The output gives us approximate probabilities assuming a normal distribution. Recall the 68‚Äì95‚Äì99.7 rule for a normal distribution: ~16% of data are above +1 SD, ~16% below ‚Äì1 SD, and only about 2.3% are above +2 SD (or below ‚Äì2 SD).\n",
    "\n",
    "From our calculations: about **0.159 (~15.9%)** of individuals have $Z>1$ (higher-than-average impulsivity), the same ~15.9% have $Z<-1$ (very low impulsivity), and only **~0.023 (2.3%)** have $Z>2$ (extremely high impulsivity). These align with our expectations for a normal distribution.\n",
    "\n",
    "---\n",
    "\n",
    "### Visualizing the Normal Distribution and Our Data\n",
    "\n",
    "To make these concepts more concrete, let's use several visualizations to connect the probability calculations to both the theoretical normal distribution and our actual data:\n",
    "\n",
    "1. **Standard Normal PDF with Shaded Area:**  \n",
    "   This plot shows the bell curve of the standard normal distribution, with the right tail shaded above a chosen z-score (e.g., the 90th percentile). This visually represents the probability of being above that threshold.\n",
    "\n",
    "2. **Standard Normal CDF with Percentile Marker:**  \n",
    "   Here, we plot the cumulative distribution function (CDF), which shows the probability of being less than or equal to a given z-score. The chosen percentile and its corresponding z-value are marked, helping you see how percentiles relate to z-scores.\n",
    "\n",
    "3. **Empirical Histogram of z-scores with Normal PDF Overlay:**  \n",
    "   This histogram displays the distribution of z-scores from our actual delay discounting data, overlaid with the standard normal curve. This helps you compare the real data‚Äôs shape to the theoretical normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76536951",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# --- 0) Prepare Delay Discounting data and z-scores (standalone) ---\n",
    "y = df['nc_y_ddis__1mo__indifpt_prop']\n",
    "y = y[(y != 999) & (~y.isna())].astype(float)\n",
    "\n",
    "mu_y, sd_y = y.mean(), y.std(ddof=1)\n",
    "zy = (y - mu_y) / sd_y\n",
    "\n",
    "# Choose a percentile and its z-cut (example: 90th)\n",
    "pct = 0.90\n",
    "z0  = norm.ppf(pct)       # ~1.2816\n",
    "x0  = mu_y + z0*sd_y      # raw-score cut in the variable's units\n",
    "\n",
    "# --- 1) Standard Normal PDF with shaded right-tail area ---\n",
    "zgrid = np.linspace(-4, 4, 800)\n",
    "pdf   = norm.pdf(zgrid)\n",
    "area  = 1 - norm.cdf(z0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(zgrid, pdf)\n",
    "plt.fill_between(zgrid, 0, pdf, where=(zgrid>=z0), alpha=0.3)\n",
    "plt.axvline(z0, ls='--', lw=1)\n",
    "plt.title(f\"Standard Normal PDF ‚Äî Shaded P(Z ‚â• {z0:.2f}) ‚âà {area:.03f}\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()\n",
    "\n",
    "# --- 2) Standard Normal CDF with percentile marker ---\n",
    "cdf = norm.cdf(zgrid)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(zgrid, cdf)\n",
    "plt.axvline(z0, ls='--', lw=1)\n",
    "plt.axhline(pct, ls='--', lw=1)\n",
    "plt.scatter([z0], [pct])\n",
    "plt.title(f\"Standard Normal CDF ‚Äî {int(pct*100)}th pct at z‚âà{z0:.2f}\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"P(Z ‚â§ z)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Empirical histogram of z-scores with Normal PDF overlay ---\n",
    "plt.figure()\n",
    "plt.hist(zy, bins=40, density=True, alpha=0.35)\n",
    "plt.plot(zgrid, pdf, lw=2)\n",
    "plt.axvline(z0, ls='--', lw=1, label=f\"{int(pct*100)}th pct z‚âà{z0:.2f}\")\n",
    "plt.title(\"Delay Discounting z-scores: Histogram + Normal PDF\")\n",
    "plt.xlabel(\"z-score of nc_y_ddis__1mo__indifpt_prop\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"{int(pct*100)}th percentile cut:\")\n",
    "print(f\"  z ‚âà {z0:.3f}\")\n",
    "print(f\"  raw score ‚âà {x0:.4f} (x = Œº + z¬∑œÉ)\")\n",
    "print(f\"  Right-tail area P(Z ‚â• {z0:.2f}) ‚âà {area:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fadaa26",
   "metadata": {},
   "source": [
    "### YT2 ‚Äî Student Choice: Normal Tail Probabilities\n",
    "\n",
    "Pick **one** continuous variable from the lists below, then:  \n",
    "1. Recode ABCD missing codes (`777, 888, 999`) to `NaN`.  \n",
    "2. Compute mean/SD and (re)standardize to z-scores.  \n",
    "3. Report Normal-model tail probabilities at three cutoffs (mirrors the demo).\n",
    "\n",
    "**Normal-friendly (good Normal fit):**  \n",
    "- `pgs_externalizing_std` (Externalizing PGS, z)  \n",
    "- `pgs_cog_std` (Cognitive PGS, z)  \n",
    "- `nc_y_nihtb__comp__tot__agecor_score` (NIH Toolbox composite) ‚Üí z-score yourself  \n",
    "- `mh_y_upps__nurg_sum` (UPPS Negative Urgency) ‚Üí z-score yourself  \n",
    "\n",
    "**Stress-test (skew/ceiling/zeros):**  \n",
    "- `nc_y_ddis__1mo__indifpt_prop` (Delay Discounting %) ‚Üí z-score yourself  \n",
    "- `nc_y_flnkr__incongr_acc` (Flanker accuracy; ceiling near 1.0) ‚Üí z-score yourself  \n",
    "- `mh_p_cbcl__synd__ext_sum` (CBCL Externalizing; zero-inflated) ‚Üí z-score yourself  \n",
    "\n",
    "**What to print (exact labels):**  \n",
    "- `VAR_CHOSEN = ...`  \n",
    "- `N_VALID = ...`  \n",
    "- `MEAN = ... ; SD = ...`  \n",
    "- `P(Z > 1) = ...   (proportion above +1 SD)`  \n",
    "- `P(Z < -1) = ...  (proportion below -1 SD)`  \n",
    "- `P(Z > 2) = ...   (proportion above +2 SD)`  \n",
    "\n",
    "*(Optional: also print empirical tail fractions from your z-scores to compare model vs. data.)*  \n",
    "\n",
    "**ü§ñ Copilot prompts (optional):**  \n",
    "- ‚ÄúShow Python code to z-score a pandas Series and print mean/SD and N after dropping 777/888/999.‚Äù  \n",
    "- ‚ÄúCompute Normal tail probabilities P(Z>1), P(Z<‚àí1), P(Z>2) using `scipy.stats.norm`.‚Äù  \n",
    "- ‚ÄúCompare empirical tail fractions from my z-scores to Normal-model expectations and print both.‚Äù  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdc27677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT2 ‚Äî Student choice: Normal tail probabilities (mirrors the demo; fill the blanks)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 1) CHOOSE ONE VARIABLE from the lists above\n",
    "VAR_NAME = '___'  # e.g., 'pgs_cog_std' or 'nc_y_nihtb__comp__tot__agecor_score'\n",
    "\n",
    "# 2) Extract and clean (ABCD-style missing ‚Üí NaN)\n",
    "series = df[___]   # TODO: insert VAR_NAME here\n",
    "series = series.replace({777: np.nan, ___: np.nan, ___: np.nan}).astype(float).dropna()\n",
    "\n",
    "# 3) Compute mean/SD and (re)standardize to z\n",
    "mean_val = series.mean()\n",
    "sd_val   = series.std(ddof=___)     # TODO: set ddof\n",
    "z_vals   = (series - ___) / ___     # TODO: subtract mean_val, divide by sd_val\n",
    "\n",
    "print(f\"VAR_CHOSEN = {VAR_NAME}\")\n",
    "print(f\"N_VALID = {z_vals.shape[0]}\")\n",
    "print(f\"MEAN = {mean_val:.3f} ; SD = {sd_val:.3f}\")\n",
    "\n",
    "# 4) Normal-model tail probabilities (same as demo)\n",
    "prob_above1     = 1 - norm.cdf(___)   # TODO: P(Z > 1)\n",
    "prob_below_neg1 = norm.cdf(-1)        # P(Z < -1)\n",
    "prob_above2     = 1 - norm.cdf(2)     # P(Z > 2)\n",
    "\n",
    "print(f\"P(Z > 1) = {prob_above1:.3f} (proportion above +1 SD)\")\n",
    "print(f\"P(Z < -1) = {prob_below_neg1:.3f} (proportion below -1 SD)\")\n",
    "print(f\"P(Z > 2) = {prob_above2:.3f} (proportion above +2 SD)\")\n",
    "\n",
    "# (Optional) Empirical tails from your standardized sample\n",
    "emp_above1  = (z_vals >= 1).mean()\n",
    "emp_below_1 = (z_vals <= -1).mean()\n",
    "emp_above2  = (z_vals >= 2).mean()\n",
    "print(f\"[Empirical] P(Z > 1) = {emp_above1:.3f} ; P(Z < -1) = {emp_below_1:.3f} ; P(Z > 2) = {emp_above2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1c0979",
   "metadata": {},
   "source": [
    "## YT3 ‚Äî Visualizing Normal Theory vs. Your Data\n",
    "\n",
    "You‚Äôll recreate three visuals from the demo for your YT2 variable:\n",
    "\n",
    "1. **Standard Normal PDF with shaded right tail at your chosen percentile‚Äôs z-cut.**\n",
    "2. **Standard Normal CDF with percentile marker (vertical and horizontal reference lines).**\n",
    "3. **Empirical histogram of your z-scores overlaid with the Standard Normal PDF (compare data vs. theory).**\n",
    "\n",
    "**Instructions:**\n",
    "- Use your YT2 objects (`series`, `mean_val`, `sd_val`, `z_vals`).\n",
    "- Pick a percentile `pct` (e.g., 0.90).\n",
    "- Compute `z0 = norm.ppf(pct)` (the z-score for your chosen percentile).\n",
    "- Back-transform to the raw cut: `x0 = mean_val + z0 * sd_val`.\n",
    "- Reproduce the three plots as shown in the demo.\n",
    "\n",
    "---\n",
    "\n",
    "### Copilot prompts (optional):\n",
    "\n",
    "- ‚ÄúHow do I use scipy.stats.norm.ppf to convert a percentile to a z-score and back-transform to a raw score?‚Äù\n",
    "- ‚ÄúShow code to shade the right tail of the Standard Normal PDF beyond a given z0 using fill_between.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cadaad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT3 ‚Äî Visuals (mirror the demo; fill the blanks)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "# Reuse your YT2 results\n",
    "zy = pd.Series(z_vals).dropna()   # standardized data (z-scores) from YT2\n",
    "\n",
    "# --- A) Choose a percentile and compute z-cut + raw cut (fill the blanks) ---\n",
    "pct = ___                 # e.g., 0.90  (the 90th percentile)\n",
    "z0  = norm.ppf(___)       # TODO: pass your percentile\n",
    "x0  = ___ + ___*___       # TODO: mean_val + z0*sd_val  (back-transform to raw)\n",
    "\n",
    "# --- B) Precompute grids and theory curves ---\n",
    "zgrid = np.linspace(-4, 4, ___)   # TODO: number of points (e.g., 800)\n",
    "pdf   = norm.pdf(zgrid)\n",
    "cdf   = norm.cdf(zgrid)\n",
    "area  = ___ - norm.cdf(z0)        # TODO: right-tail area (use 1 - norm.cdf(z0))\n",
    "\n",
    "# --- 1) Standard Normal PDF with shaded right-tail area ---\n",
    "plt.figure()\n",
    "plt.plot(zgrid, pdf)\n",
    "plt.fill_between(zgrid, 0, pdf, where=(zgrid >= z0), alpha=0.3)\n",
    "plt.axvline(z0, ls='--', lw=1)\n",
    "# TODO: add a descriptive title showing z0 and area (see demo)\n",
    "# plt.title(f\"Standard Normal PDF ‚Äî Shaded P(Z ‚â• {z0:.2f}) ‚âà {area:.03f}\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.show()\n",
    "\n",
    "# --- 2) Standard Normal CDF with percentile marker ---\n",
    "plt.figure()\n",
    "plt.plot(zgrid, cdf)\n",
    "plt.axvline(z0, ls='--', lw=1)\n",
    "plt.axhline(pct, ls='--', lw=1)\n",
    "# TODO: place the marker point at (z0, pct)\n",
    "# plt.scatter([z0], [pct])\n",
    "# TODO: add a title showing the percentile and z0 (see demo)\n",
    "# plt.title(f\"Standard Normal CDF ‚Äî {int(pct*100)}th pct at z‚âà{z0:.2f}\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"P(Z ‚â§ z)\")\n",
    "plt.show()\n",
    "\n",
    "# --- 3) Empirical histogram of z-scores with Normal PDF overlay ---\n",
    "plt.figure()\n",
    "plt.hist(zy, bins=___, density=___, alpha=0.35)   # TODO: choose bins (e.g., 40) and set density=True\n",
    "plt.plot(zgrid, pdf, lw=2)\n",
    "plt.axvline(z0, ls='--', lw=1, label=f\"{int(pct*100)}th pct z‚âà{z0:.2f}\")\n",
    "plt.title(\"Your variable (z): Histogram + Normal PDF\")\n",
    "plt.xlabel(f\"z-score of {VAR_NAME}\")\n",
    "plt.ylabel(\"density\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Printed summary (like the demo)\n",
    "print(f\"{int(pct*100)}th percentile cut:\")\n",
    "print(f\"  z ‚âà {z0:.3f}\")\n",
    "print(f\"  raw score ‚âà {x0:.4f} (x = Œº + z¬∑œÉ)\")\n",
    "print(f\"  Right-tail area P(Z ‚â• {z0:.2f}) ‚âà {area:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3893073",
   "metadata": {},
   "source": [
    "### Activity 3. Binomial Distribution: Substance Use in a Sample\n",
    "\n",
    "**Demo:** The **binomial distribution** models the number of ‚Äúsuccesses‚Äù in a fixed number of independent trials, given a constant probability of success on each trial. In our context, we can use a binomial model to answer questions like: *‚ÄúWhat is the probability that in a group of $n$ people, exactly $k$ of them have engaged in a certain substance use behavior?‚Äù* or *‚ÄúWhat is the probability at least one person in the group has tried substance X?‚Äù*\n",
    "\n",
    "Let‚Äôs demonstrate this using **alcohol use (sip)**. The variable `su_y_sui__use__alc__sip_001__l` indicates whether the participant has ever had even a sip of alcohol (0 = No, 1 = Yes). First, we need the probability $p$ that any one individual has had an alcohol sip. We can estimate $p$ from our data as the sample proportion of 1s in that column. Then we‚Äôll consider a **small sample** of $n=5$ people (e.g., a group of 5 friends) and use the binomial distribution to calculate probabilities.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8af569c",
   "metadata": {},
   "source": [
    "**A. Estimate \\( p \\) from data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11871117",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose the binary variable (0/1; sip of alcohol)\n",
    "col = \"su_y_sui__use__alc__sip_001__l\"\n",
    "x = df[col].dropna().astype(float)\n",
    "p = x.mean()        # estimated individual probability\n",
    "n = 5               # sample size (group of 5)\n",
    "\n",
    "print(f\"VARIABLE = {col}\")\n",
    "print(f\"p = {p:.3f} ; n = {n}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b342f4f",
   "metadata": {},
   "source": [
    "#### B. Show the binomial PMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b836a",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(n+1)\n",
    "pmf = binom.pmf(k, n, p)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(k, pmf, edgecolor=\"black\")\n",
    "plt.xlabel(\"k = number in group with a sip (out of n)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Binomial PMF ‚Äî Alcohol Sip (p={p:.2f}, n={n})\")\n",
    "plt.xticks(k)\n",
    "plt.show()\n",
    "\n",
    "# one-line methods caption students can reuse\n",
    "print(f\"Methods: p estimated as sample mean of {col}; Binomial PMF shown for k=0..{n}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760f114",
   "metadata": {},
   "source": [
    "#### C. Compute key probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5874a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "P0 = binom.pmf(0, n, p)         # none\n",
    "P_ge1 = 1 - P0                   # at least one\n",
    "k_exact = 3                      # editable: exactly 3\n",
    "Pk = binom.pmf(k_exact, n, p)\n",
    "\n",
    "print(f\"P(0 of {n})   = {P0:.3f}\")\n",
    "print(f\"P(‚â•1 of {n})  = {P_ge1:.3f}\")\n",
    "print(f\"P(exactly {k_exact} of {n}) = {Pk:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11d2de4",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "\n",
    "Think of the bars as answers to:  \n",
    "‚ÄúOut of a group of $n$ people, how many will have had a sip?‚Äù  \n",
    "Each bar at $k$ shows the chance of seeing exactly $k$ people with a sip.\n",
    "\n",
    "With $p \\approx {p:.2f}$ and $n = {n}$, the tallest bar is near $k \\approx n p$ (here, around $k = \\mathrm{int}(\\mathrm{round}(\\mathrm{float}(p) \\times n))$).  \n",
    "That tallest bar marks the most likely headcount in a group of size $n$.\n",
    "\n",
    "The leftmost bar ($k = 0$) is the chance that no one in the group has had a sip:  \n",
    "$P(X = 0) = {P0:.3f}$.  \n",
    "Everything to the right of that bar represents ‚Äúat least one,‚Äù which is why  \n",
    "$1 - P(X = 0) = {P_ge1:.3f}$.\n",
    "\n",
    "If you look at the bar at $k = {k_exact}$, that single bar corresponds to the chance of seeing exactly that many people with a sip:  \n",
    "$P(X = {k_exact}) = {Pk:.3f}$.\n",
    "\n",
    "As $p$ gets larger (each person is more likely), the whole set of bars shifts to the right and $P(X \\ge 1)$ gets closer to 1.  \n",
    "When $p$ is smaller, most of the mass stays on the left, especially the $k = 0$ bar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b41da6f",
   "metadata": {},
   "source": [
    "### Your Turn: Binomial Model on a Binary Outcome (Student Choice)\n",
    "\n",
    "**Goal (same as the demo):** Estimate an individual probability \\(p\\) from a 0/1 column and use the binomial model \\(X\\sim \\text{Binomial}(n,p)\\) to compute and **visualize** probabilities for a small group.\n",
    "\n",
    "**Choose ONE binary variable (0/1):**\n",
    "- **Core (recommended; no/low missing):**  \n",
    "  `su_y_sui__use__mj__puff_001__l` (Marijuana puff),  \n",
    "  `su_y_sui__use__alc__sip_001__l` (Alcohol sip),  \n",
    "  `su_y_sui__use__nic__puff_001__l` (Nicotine puff/vape).\n",
    "- **Optional/Advanced (recode 777/999‚ÜíNaN before computing):**  \n",
    "  `mh_p_famhx__alc_001__v02`, `mh_p_famhx__drg_001__v02`, `mh_p_famhx__troub_001__v02`.\n",
    "\n",
    "**What to do (mirror the demo):**\n",
    "1) Compute \\(p\\) as the sample mean of your chosen column (after cleaning, if needed).  \n",
    "2) Set a small group size \\(n\\) (use \\(n=5\\) to match the demo).  \n",
    "3) Make the **Binomial PMF** bar chart for \\(k=0..n\\) with a clear title/axes.  \n",
    "4) Compute and print: \\(P(X=0)\\) (none), \\(P(X\\ge 1)=1-P(X=0)\\) (at least one), and \\(P(X=k_{\\text{exact}})\\) for a chosen \\(k\\).  \n",
    "5) Interpret using the figure (where is the tallest bar ‚âà \\(np\\)? Is ‚Äúnone‚Äù plausible? How big is ‚Äúat least one‚Äù?).\n",
    "\n",
    "**ü§ñ Copilot prompts (optional):**\n",
    "- ‚ÄúSelect a binary column in pandas and recode 777/999 to NaN before computing the mean.‚Äù  \n",
    "- ‚ÄúUse `scipy.stats.binom` to compute a PMF for k=0..n and plot it with matplotlib bars.‚Äù  \n",
    "- ‚ÄúCompute P(X=0), P(X‚â•1) as a complement, and P(X=k_exact) for a binomial distribution.‚Äù\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0becfd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YT ‚Äî Binomial PMF (mirror the demo; fill the blanks)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import binom\n",
    "\n",
    "# 1) Choose ONE binary variable from the list in the markdown\n",
    "col = '___'   # e.g., 'su_y_sui__use__mj__puff_001__l' or 'su_y_sui__use__alc__sip_001__l'\n",
    "\n",
    "# 2) Extract and clean (advanced vars may use 777/999 codes; harmless for core vars)\n",
    "x = df[col].replace({777: np.nan, 999: np.nan}).dropna().astype(float)\n",
    "\n",
    "# 3) Estimate p and set group size n\n",
    "p = x.mean()          # estimated individual probability\n",
    "n = ___               # TODO: set small sample size (use 5 to mirror the demo)\n",
    "\n",
    "print(f\"VARIABLE = {col}\")\n",
    "print(f\"p = {p:.3f} ; n = {n}\")\n",
    "\n",
    "# 4) Binomial PMF and bar chart\n",
    "k   = np.arange(n + 1)\n",
    "pmf = binom.pmf(k, n, p)\n",
    "\n",
    "plt.figure()\n",
    "plt.bar(k, pmf, edgecolor=\"black\")\n",
    "plt.xlabel(\"k = number in group with the behavior (out of n)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Binomial PMF ‚Äî ___ (p={p:.2f}, n={n})\")   # TODO: replace ___ with a short label (e.g., 'Alcohol Sip' or 'Marijuana Puff')\n",
    "plt.xticks(k)\n",
    "plt.show()\n",
    "\n",
    "# One-line methods caption (students can reuse in reflection)\n",
    "print(f\"Methods: p estimated as sample mean of {col}; Binomial PMF shown for k=0..{n}.\")\n",
    "\n",
    "# 5) Key probabilities\n",
    "P0       = binom.pmf(0, n, p)      # none\n",
    "P_ge1    = ___                     # TODO: complement of P0 (at least one)\n",
    "k_exact  = ___                     # TODO: choose an exact count (e.g., 2 or 3)\n",
    "Pk       = binom.pmf(k_exact, n, p)\n",
    "\n",
    "print(f\"P(0 of {n})   = {P0:.3f}\")\n",
    "print(f\"P(‚â•1 of {n})  = {P_ge1:.3f}\")\n",
    "print(f\"P(exactly {k_exact} of {n}) = {Pk:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862678a4",
   "metadata": {},
   "source": [
    "### Activity 4. Poisson Distribution: Simulating Overdose Counts\n",
    "\n",
    "**Demo:** The **Poisson distribution** is useful for modeling counts of rare events over a given time or space, especially when the events occur independently. In behavioral genetics and public health, we might use a Poisson model for something like the number of **opioid overdose events** in a population within a year. The Poisson distribution has a single parameter $\\lambda$ (lambda), which is both the expected value *and* the variance of the distribution. For example, if on average 10 overdoses occur per year in a city, we could model the yearly count of overdoses as $\text{Pois}(\\lambda=10)$.\n",
    "\n",
    "Let‚Äôs simulate a scenario: suppose the **annual overdose rate** is approximately ‚Äú$\\mathbf{0.001}$ per person per year‚Äù (this would correspond to about 0.1% risk per person per year, or 1 overdose per 1000 people per year ‚Äì a rate in the range of reported values for opioid overdoses in some populations). If our synthetic sample has about 10,000 individuals, the expected number of overdoses in one year for this group is $\\lambda = 10{,}000 \times 0.001 = 10$. We will simulate the distribution of the number of overdose events in one year across many hypothetical ‚Äúrepetitions‚Äù of that year.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14c0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define rate and population\n",
    "rate_per_person = 0.001   # 0.1% chance of an overdose per person-year\n",
    "population = 10000       # consider 10,000 people\n",
    "lam = rate_per_person * population  # expected number of overdoses in the group per year\n",
    "\n",
    "# Simulate the number of overdose events over 10,000 hypothetical one-year trials\n",
    "num_simulations = 10000\n",
    "counts = np.random.poisson(lam, size=num_simulations)\n",
    "\n",
    "# Approximate probabilities from simulation for a range of counts\n",
    "# e.g., frequency of seeing 0 overdoses, 1 overdose, 2 overdoses, ... etc.\n",
    "vals, freqs = np.unique(counts, return_counts=True)\n",
    "prob_est = freqs / num_simulations\n",
    "\n",
    "# Print some example probabilities from the simulation\n",
    "for k in range(0, 6):  # 0 through 5 overdoses\n",
    "    p_k = prob_est[vals.tolist().index(k)] if k in vals else 0\n",
    "    print(f\"P(X = {k} overdoses) ‚âà {p_k:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0efa7a",
   "metadata": {},
   "source": [
    "The above simulation allows us to estimate probabilities like $P(X=0)$, $P(X=1)$, etc., where $X$ = number of overdoses in a year in this population. We expect $P(X=10)$ to be the highest (since $\\lambda=10$), and probabilities will drop off for much larger or much smaller counts.\n",
    "\n",
    "Let‚Äôs also visualize the **Poisson distribution** of $X$ with $\\lambda = 10$: we‚Äôll plot the probability mass for counts. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b5422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Limit to a reasonable range for plotting (say 0 to 20 overdoses)\n",
    "max_k = 20\n",
    "vals_plot = vals[vals <= max_k]\n",
    "probs_plot = prob_est[vals <= max_k]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(vals_plot, probs_plot, width=0.8, color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(0, max_k+1, 2))\n",
    "plt.xlabel(\"Number of opioid overdose events in one year (N=10,000 people)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Distribution of annual overdose counts (Œª = {lam:.1f})\")\n",
    "savefig(\"poisson_overdose_distribution.png\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39773460",
   "metadata": {},
   "source": [
    "*Simulated Poisson distribution of yearly opioid overdose counts for a population of 10,000 (assuming an average rate $\\lambda=10$ per year).* The bar heights represent the probability of seeing that many overdose events in a year. The distribution is centered around 10 (the expected value) and has a roughly bell-shaped spread: for example, 8, 9, 10, 11 overdoses are the most likely counts in a year, each with around 10‚Äì12% probability. Lower counts like 0, 1, 2 are extremely unlikely (less than 0.1% for 0 events, in this simulation), and very high counts (say 20 or more) are also unlikely. This reflects the Poisson property that the variance equals the mean ‚Äì there‚Äôs considerable year-to-year fluctuation, but it‚Äôs relatively rare to deviate by more than a few events from the mean in either direction.\n",
    "\n",
    "### Your Turn: Adjusting Rates or Population in Poisson\n",
    "\n",
    "Now, experiment with the Poisson model by changing parameters:  \n",
    "\n",
    "- **Try a different rate:** For instance, what if the overdose rate were higher, say 0.002 per person (0.2%)? Recalculate $\\lambda$ for the same population of 10,000 (it would double to 20) and simulate again. How does the distribution change (is it wider, more skewed)? What‚Äôs the probability of 0 overdoses now (it will be even smaller)?  \n",
    "\n",
    "- **Try a different population size:** If we only had 1,000 people instead of 10,000 (keeping the original 0.1% rate), $\\lambda$ would drop to 1.0. That yields a $\text{Pois}(1)$ distribution, which is much more likely to have 0 events in a year. You can simulate this scenario and observe that $P(X=0)$ might be around 37% when $\\lambda=1$.  \n",
    "\n",
    "Feel free to adjust `rate_per_person` or `population` in the code and re-run it. Then inspect the probabilities or plot: see how a larger $\\lambda$ (due to higher rate or more people) makes the Poisson distribution spread out more (and the chance of zero events drops), whereas a smaller $\\lambda$ makes the distribution concentrate more at the lower counts (including a higher chance of zero events).\n",
    "\n",
    "**ü§ñ Copilot prompts (optional):**\n",
    "\n",
    " - ‚ÄúSelect a binary column in pandas and recode 777/999 to NaN before computing the mean.‚Äù\n",
    " - ‚ÄúUse scipy.stats.binom to compute a PMF for k=0..n and plot it with matplotlib bars.‚Äù\n",
    " - ‚ÄúCompute P(X=0), P(X‚â•1) as a complement, and P(X=k_exact) for a binomial distribution.‚Äù"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc23c023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define rate and population\n",
    "rate_per_person = ____   # e.g., 0.001 for 0.1% chance per person-year\n",
    "population = ____        # e.g., 10000 people\n",
    "lam = rate_per_person * population  # expected number of overdoses in the group per year\n",
    "\n",
    "# Simulate the number of overdose events over ____ hypothetical one-year trials\n",
    "num_simulations = ____   # e.g., 10000\n",
    "counts = np.random.poisson(lam, size=num_simulations)\n",
    "\n",
    "# Approximate probabilities from simulation for a range of counts\n",
    "vals, freqs = np.unique(counts, return_counts=True)\n",
    "prob_est = freqs / num_simulations\n",
    "\n",
    "# Print some example probabilities from the simulation\n",
    "for k in range(0, ____):  # e.g., 0 through 5 overdoses\n",
    "    p_k = prob_est[vals.tolist().index(k)] if k in vals else 0\n",
    "    print(f\"P(X = {k} overdoses) ‚âà {p_k:.3f}\")\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Limit to a reasonable range for plotting (e.g., 0 to 20 overdoses)\n",
    "max_k = ____\n",
    "vals_plot = vals[vals <= max_k]\n",
    "probs_plot = prob_est[vals <= max_k]\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.bar(vals_plot, probs_plot, width=0.8, color='skyblue', edgecolor='black')\n",
    "plt.xticks(range(0, max_k+1, 2))\n",
    "plt.xlabel(f\"Number of opioid overdose events in one year (N={population} people)\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.title(f\"Distribution of annual overdose counts (Œª = {lam:.1f})\")\n",
    "savefig(\"poisson_overdose_distribution_adjusted.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73183263",
   "metadata": {},
   "source": [
    "### Optional Activity 3 Extension: Binomial (with integrated Bernoulli micro-lesson)\n",
    "\n",
    "**Learning goal.** Model counts of ‚Äúsuccesses‚Äù (yes/no outcomes) across a small group using the Binomial distribution, and recognize the Bernoulli as the single-trial special case.\n",
    "\n",
    "> #### 60-second micro-callout\n",
    ">\n",
    "> **Bernoulli**$(p)$ **‚â°** **Binomial**$(n = 1, p)$.\n",
    "> Each person‚Äôs yes/no is a Bernoulli; counts across a small group follow a Binomial.\n",
    "\n",
    "**Intuition.** If each individual has probability $p$ of ‚Äúyes‚Äù (independent trials, same $p$), then for a group of size $n$, the total number of ‚Äúyes‚Äù responses $X$ is\n",
    "\n",
    "$$\n",
    "X \\sim \\mathrm{Binomial}(n, p),\\quad \n",
    "P(X=k)=\\binom{n}{k}p^{k}(1-p)^{n-k},\\; k=0,1,\\dots,n.\n",
    "$$\n",
    "\n",
    "Special case $n=1$: a single trial is Bernoulli$(p)$, producing 0/1.\n",
    "\n",
    "---\n",
    "\n",
    "#### Minimal examples (copy into your notebook)\n",
    "\n",
    "**A. Single individuals (Bernoulli as Binomial n=1).**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a9307a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "p = 0.25                      # probability of \"yes\" per individual\n",
    "N = 10_000                    # number of people\n",
    "single_trials = np.random.binomial(n=1, p=p, size=N)  # 0/1 draws\n",
    "\n",
    "prop_yes = single_trials.mean()\n",
    "print(f\"Observed P(yes) ‚âà {prop_yes:.3f} (theoretical p = {p})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f34eb0e",
   "metadata": {},
   "source": [
    "**B. Small groups (Binomial counts).** Suppose we form groups of size `n=5` and count how many ‚Äúyes‚Äù per group.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf47205c",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5\n",
    "G = 5_000                     # number of groups\n",
    "counts = np.random.binomial(n=n, p=p, size=G)  # total yes per group\n",
    "\n",
    "# Empirical distribution of counts\n",
    "vals, freqs = np.unique(counts, return_counts=True)\n",
    "pmf_emp = freqs / G\n",
    "print(\"k   Empirical P(X=k)\")\n",
    "for k, pk in zip(vals, pmf_emp):\n",
    "    print(f\"{k:<2d}  {pk:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01098fcf",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**C. Common probabilities.** With $X\\sim \\mathrm{Binomial}(n,p)$:\n",
    "\n",
    "* ‚ÄúNone‚Äù: $P(X=0)=(1-p)^n$\n",
    "* ‚ÄúAt least one‚Äù: $P(X\\ge 1)=1-(1-p)^n$\n",
    "* ‚ÄúExactly $k$‚Äù: $P(X=k)=\\binom{n}{k}p^k(1-p)^{n-k}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c2990",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import comb\n",
    "\n",
    "def binom_pmf(n, p, k):\n",
    "    return comb(n, k) * (p**k) * ((1-p)**(n-k))\n",
    "\n",
    "n = 5; p = 0.25\n",
    "p_none        = (1-p)**n\n",
    "p_at_least_1  = 1 - (1-p)**n\n",
    "p_exactly_2   = binom_pmf(n, p, 2)\n",
    "\n",
    "print(f\"P(X=0)         = {p_none:.3f}\")\n",
    "print(f\"P(X‚â•1)         = {p_at_least_1:.3f}\")\n",
    "print(f\"P(X=2)         = {p_exactly_2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d87154c",
   "metadata": {},
   "source": [
    "#### Takeaway\n",
    "\n",
    "* Treat each person‚Äôs response as **Bernoulli(p)**.\n",
    "* Aggregate to a group of size **n** ‚Üí total ‚Äúyes‚Äù is **Binomial(n, p)**.\n",
    "* Use closed-form Binomial probabilities for ‚Äúnone,‚Äù ‚Äúat least one,‚Äù and ‚Äúexactly k,‚Äù or simulate to build intuition."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
